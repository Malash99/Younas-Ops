# Scalable Multi-Camera Underwater Visual Odometry with Transformers

## Project Overview

This project develops a novel **scalable multi-camera transformer architecture** for underwater visual odometry that can adaptively work with 1-5 cameras while incorporating IMU and barometer data. The goal is to create a robust, generalizable system that performs well across different camera configurations and can handle sensor failures.

## Key Innovation: Adaptive Multi-Camera Architecture

Unlike traditional fixed-camera setups, our approach dynamically adapts to available cameras, making it practical for real-world underwater robotics where camera failures are common.

## Research Questions

1. **How does camera count affect underwater VO performance?**
2. **What's the optimal camera selection strategy for different underwater conditions?**
3. **How do sequence length and multi-modal fusion impact accuracy?**
4. **Can the model generalize across different underwater trajectories?**

## Architecture: UW-TransVO (Underwater Transformer Visual Odometry)

### Core Components

```
Input: Variable cameras (1-5) Ã— sequence_length (2 or 4) Ã— 224Ã—224Ã—3 + IMU + Pressure

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UW-TransVO Architecture                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Underwater Image Enhancement Module                      â”‚
â”‚    â”œâ”€â”€ Color correction for underwater conditions           â”‚
â”‚    â”œâ”€â”€ Contrast enhancement                                 â”‚
â”‚    â””â”€â”€ Noise reduction                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Multi-Camera Feature Extraction                         â”‚
â”‚    â”œâ”€â”€ Shared CNN backbone (ResNet50/EfficientNet)         â”‚
â”‚    â”œâ”€â”€ Camera-specific positional encoding                 â”‚
â”‚    â””â”€â”€ Feature dimension: 768                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. Spatial Cross-Camera Attention                          â”‚
â”‚    â”œâ”€â”€ Attention between simultaneous camera views         â”‚
â”‚    â”œâ”€â”€ Handles variable number of cameras (1-5)            â”‚
â”‚    â””â”€â”€ Camera masking for missing inputs                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. Temporal Self-Attention                                 â”‚
â”‚    â”œâ”€â”€ Attention across time steps (2 or 4 frames)         â”‚
â”‚    â”œâ”€â”€ Motion-aware attention weights                      â”‚
â”‚    â””â”€â”€ Positional encoding for temporal relationships      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 5. Multi-Modal Sensor Fusion                               â”‚
â”‚    â”œâ”€â”€ IMU integration (acceleration + gyroscope)          â”‚
â”‚    â”œâ”€â”€ Pressure/depth information                          â”‚
â”‚    â””â”€â”€ Cross-modal attention between vision and sensors    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 6. 6-DOF Pose Regression Head                              â”‚
â”‚    â”œâ”€â”€ Translation: (Î”x, Î”y, Î”z)                          â”‚
â”‚    â”œâ”€â”€ Rotation: (Î”roll, Î”pitch, Î”yaw)                    â”‚
â”‚    â””â”€â”€ Uncertainty estimation                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Output: 6-DOF relative pose + uncertainty estimates
```

## Experimental Design

### Dataset Configuration
- **Training Dataset**: Ariel underwater vehicle (5-camera setup)
- **Validation Dataset**: Same trajectory, different time segment
- **Test Dataset 1**: Unseen trajectory from same platform
- **Test Dataset 2**: Additional downloaded dataset (generalizability test)

### Camera Configurations
```python
CAMERA_CONFIGS = {
    'monocular': [0],                    # Single camera
    'stereo_wide': [0, 2],              # Wide baseline stereo
    'stereo_close': [0, 1],             # Close baseline stereo  
    'triple': [0, 1, 2],                # Triple camera setup
    'quad': [0, 1, 2, 3],              # Quad camera setup
    'full': [0, 1, 2, 3, 4]            # Full 5-camera array
}
```

### Sequence Configurations
```python
SEQUENCE_CONFIGS = {
    'short': 2,     # 2-frame sequences
    'long': 4       # 4-frame sequences  
}
```

### Multi-Modal Configurations
```python
MODALITY_CONFIGS = {
    'vision_only': ['cameras'],
    'vision_imu': ['cameras', 'imu'],
    'vision_pressure': ['cameras', 'pressure'],
    'vision_all': ['cameras', 'imu', 'pressure']
}
```

## Complete Experiment Matrix

| Model | Cameras | Sequence | Modality | Total Experiments |
|-------|---------|----------|----------|-------------------|
| UW-TransVO | 6 configs | 2 lengths | 4 modalities | **48 experiments** |

### Ablation Studies
1. **Camera Count Impact**: Performance vs number of cameras
2. **Sequence Length**: 2-frame vs 4-frame sequences
3. **Sensor Fusion**: Vision-only vs multi-modal
4. **Camera Selection**: Optimal camera combinations
5. **Robustness**: Performance with missing cameras

## Loss Functions

### Weighted Multi-Task Loss
```python
L_total = Î»_trans * L_translation + Î»_rot * L_rotation + Î»_uncert * L_uncertainty

Where:
- Î»_trans = 1.0      # Translation weight
- Î»_rot = 10.0       # Rotation weight (higher due to scale difference)
- Î»_uncert = 0.1     # Uncertainty regularization
```

### Individual Loss Components
```python
L_translation = MSE(pred_xyz, gt_xyz)
L_rotation = MSE(pred_rpy, gt_rpy) 
L_uncertainty = KL_divergence(pred_uncertainty, target_uncertainty)
```

## Evaluation Metrics

### Trajectory Metrics
- **ATE (Absolute Trajectory Error)**: Global position accuracy
- **RPE (Relative Pose Error)**: Local motion accuracy
- **Rotation Error**: Angular accuracy in degrees
- **Scale Drift**: Long-term trajectory consistency

### Underwater-Specific Metrics
- **Performance vs Visibility**: Error correlation with image quality
- **Depth Accuracy**: Pressure sensor fusion effectiveness
- **Motion Type Analysis**: Performance on different motion patterns

### Generalizability Metrics
- **Cross-Dataset Performance**: Model trained on dataset A, tested on B
- **Camera Failure Robustness**: Performance with missing cameras
- **Trajectory Length**: Accuracy vs sequence length

## Training Strategy & Data Processing

### **Key Training Innovation: Shuffled Frame Pairs**

Instead of sequential frame pairs (1,2), (2,3), (3,4)..., we use **randomized frame pairs** from across the entire dataset:
- Frame pairs: (12,13), (156,157), (89,90), (234,235)...
- **Benefits:**
  - Forces model to learn **general motion patterns** rather than memorizing sequences
  - **Better generalization** across different motion types
  - **Prevents overfitting** to specific trajectory patterns
  - **More robust** to various underwater conditions

### Training Data Structure
```
data/processed/training_dataset/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ cam0/          # 27,399 images
â”‚   â”œâ”€â”€ cam1/          # 27,399 images  
â”‚   â”œâ”€â”€ cam2/          # 27,399 images
â”‚   â”œâ”€â”€ cam3/          # 27,399 images
â”‚   â””â”€â”€ cam4/          # 27,399 images
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ training_data.csv      # All modalities + ground truth
â”‚   â””â”€â”€ metadata.json          # Dataset configuration
â””â”€â”€ splits/
    â”œâ”€â”€ train_samples.txt      # Bags 0-2 (training)
    â”œâ”€â”€ val_samples.txt        # Bag 3 (validation)  
    â””â”€â”€ test_samples.txt       # Bag 4 (test)
```

### Ground Truth Format
```csv
sample_id,bag_name,timestamp,dt,delta_x,delta_y,delta_z,delta_roll,delta_pitch,delta_yaw,
pose_x,pose_y,pose_z,imu_accel_x,imu_accel_y,imu_accel_z,imu_gyro_x,imu_gyro_y,imu_gyro_z,
pressure,cam0_path,cam1_path,cam2_path,cam3_path,cam4_path
```

## Implementation Plan

### Phase 1: Core Architecture âœ… **COMPLETED**
- âœ… Scalable transformer architecture (1-5 cameras)
- âœ… Multi-camera spatial attention
- âœ… Temporal self-attention 
- âœ… Multi-modal sensor fusion
- âœ… Underwater image enhancement
- âœ… Adaptive data loading with camera masks

### Phase 2: Training Pipeline ğŸš§ **IN PROGRESS**
- âœ… Shuffled frame pair strategy
- ğŸ”„ Loss functions with uncertainty weighting
- ğŸ”„ Multi-task optimization
- ğŸ”„ Experiment configuration framework
- â³ Training loop with multiple camera configs

### Phase 3: Evaluation Framework â³ **NEXT**
- â³ Trajectory visualization (global frame)
- â³ Comprehensive metrics (ATE, RPE)
- â³ Ablation study framework
- â³ Baseline comparisons

### Phase 4: Generalizability Testing â³ **PLANNED**
- â³ Cross-dataset validation
- â³ Camera failure simulation
- â³ Performance analysis
- â³ Paper preparation

## Expected Contributions

### Technical Contributions
1. **Novel Architecture**: First scalable multi-camera transformer for underwater VO
2. **Adaptive Attention**: Camera-agnostic attention mechanism
3. **Multi-Modal Fusion**: Effective integration of vision, IMU, and pressure data
4. **Robustness**: Handle sensor failures gracefully

### Scientific Contributions  
1. **Comprehensive Analysis**: Camera count vs performance trade-offs
2. **Generalizability Study**: Cross-dataset validation methodology
3. **Underwater-Specific Insights**: Domain-specific challenges and solutions
4. **Practical Guidelines**: Camera selection strategies for underwater robots

## Publication Strategy

### Target Venues
- **Primary**: ICRA 2025, IROS 2025
- **Secondary**: IEEE Robotics and Automation Letters (RA-L)
- **Domain-specific**: OCEANS Conference, Autonomous Robots Journal

### Paper Structure
```
1. Introduction
   - Underwater VO challenges
   - Multi-camera system motivation
   
2. Related Work
   - Visual odometry methods
   - Transformer architectures
   - Underwater robotics
   
3. Method: UW-TransVO
   - Architecture details
   - Multi-camera adaptation
   - Multi-modal fusion
   
4. Experiments
   - Dataset description
   - Ablation studies
   - Generalizability tests
   
5. Results & Analysis
   - Performance comparison
   - Camera count analysis
   - Failure case studies
   
6. Conclusion & Future Work
```

## Getting Started

### Prerequisites
```bash
pip install torch torchvision transformers
pip install opencv-python numpy pandas matplotlib
pip install scikit-learn tqdm tensorboard
```

### Quick Start
```bash
# 1. Extract training data
python scripts/extract_all_data.py --output_dir data/processed/full_dataset

# 2. Train baseline model (quad cameras, vision only)
python experiments/train_transformer.py --cameras 0,1,2,3 --sequence_length 2 --modality vision_only

# 3. Evaluate and visualize
python experiments/evaluate_model.py --model_path models/quad_seq2_vision.pth --plot_trajectory
```

---

**This approach provides a solid foundation for a high-impact publication in underwater visual odometry, with novel technical contributions and comprehensive experimental validation.**